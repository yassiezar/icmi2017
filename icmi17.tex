\documentclass[format=sigconf, review=true, screen=true, anonymous=true]{acmart}

\usepackage{siunitx}
\usepackage{graphicx}

\begin{document}

\acmConference[ICMI'17]{International Conference on Multimodal Interaction}{November 2017}{Glascow, Scotland}

\title[SHORT TITLE]{LONG TITLE}
\subtitle{SUBTITLE}

\author{Jacobus C. Lock}
\affiliation{%
  \institution{University of Lincoln}
  \department{School of Computer Science}
  \streetaddress{Brayford Pool}
  \city{Lincoln}
  \state{Lincolnshire}
  \postcode{LN6 7TS}
  \country{United Kingdom}}
\email{jlock@lincoln.ac.uk}

\author{Grzegorz Cielniak}
\affiliation{%
  \institution{University of Lincoln}
  \department{School of Computer Science}
  \streetaddress{Brayford Pool}
  \city{Lincoln}
  \state{Lincolnshire}
  \postcode{LN6 7TS}
  \country{United Kingdom}}
\email{gcielniak@lincoln.ac.uk}

\author{Nicola Bellotto}
\affiliation{%
  \institution{University of Lincoln}
  \department{School of Computer Science}
  \streetaddress{Brayford Pool}
  \city{Lincoln}
  \state{Lincolnshire}
  \postcode{LN6 7TS}
  \country{United Kingdom}}
\email{nbellotto@lincoln.ac.uk}

\thanks{THANKS}

\terms{KEY TERMS}
\keywords{Human-machine interface, multi-modal HMI, visually impaired, navigation, non-verbal instructions, spatialised sound, varying pitch}

\acmYear{2017}
\received{May 2017}

\maketitle

\begin{abstract}
  hello
\end{abstract}

\section{Introduction}

The UK's Royal National Institute for the Blind (RNIB), a leading organisation in the area, has identified a number of challenges for the modern blind and visually impaired (henceforth referred to as the VI) person. These include the latter's ability to safely and independently use public transport and navigate in unfamiliar environments~\cite{rnib-objectives}. Recent technological advances in the fields of mobile computing and computer vision have allowed for new and innovative solutions to come to the fore to address these challenges. 

To this end, we propose a mobile device-based navigation system that caters to the needs of the blind and VI that is based on a Google Project Tango device. A Tango-enabled device comes pre-equipped with powerful image-processing, localisation and depth-perception capabilities and is built on top of a standard Android platform, providing access to the entire set of input/output options that Android has to offer. The proposed system will use multiple feedback modes to guide a user toward a target destination while providing information on any oncoming obstacles.

In this paper, we discuss in detail how one of these feedback modes are used in our system. We also discuss the experiments we performed to determine how effective this mode is at directing a user to completing a given task, as well as how its parameter values affect a user's performance at completing the aforementioned task. 

The contributions to the current body of knowledge are two-fold: 

\begin{enumerate}
  \item these are the first experimental results obtained with a significant sample size that provide an indication on how well a tone with varying pitch can convey the elevation angle of a target and 
  \item the system is based on a hand-held mobile device that does all of the processing on-board and uses a nondescript, hand-manipulated camera instead of a head-mounted camera sensor 
\end{enumerate}

The remainder of this paper is organised as follows: work previously performed that is relevant to this work is presented and discussed, followed by a description of the navigation system and its audio feedback interface. The experiments we performed and the results it generated are then discussed. A brief summary of the current work and an overview of the future work is then given in conclusion. 

\section{Previous Work}

In the past there has been much academic and commercial interest in the problem of empowering the blind and visually impaired to use the tools and systems normally-sighted people take for granted and to allow them to play a more active role in modern society. In this section we discuss some of the relevant work that has been done to enable this. 

\subsection{Navigation}

Delivering a system that will allow the blind and visually impaired (VI) to independently navigate and accomplish everyday tasks is not a new proposal; in fact, there are multiple commercial systems available (besides the traditional walking cane) and academic research for this field dates as far back as [CITE WHEN]. The products vary from sonar, radar and GPS-based systems, to some of the more recent systems which use computer vision techniques to detect and avoid obstacles in the user's path. 

One approach that has been investigated is to outfit the existing white walking cane with various sensors, such as sonar, radar, motor encoders, etc.,~\cite{ulrich1997, marion2008batcane} to detect warn the user of upcoming obstacles from a distance instead of relying on haptic feedback from the impact between the cane and the obstacle. These systems are fairly simple and reliable and are familiar to the VI, but they are typically clunky, advertising the users' disability, which can be a major hurdle to market penetration. Another, more discreet approach, was to equip a normal to act as an radio-frequency identification (RFID) antennae to read a set of RFID tags that are placed around the environment at key spots or along a path~\cite{faria2010electronic, willis2005}. This approach to modifying the traditional cane is more discreet than the systems mentioned earlier and has been shown to work well. However, the major drawback here is the significant cost of modifying existing infrastructure with RFID tags and maintaining them to keep up with a changing environment. GPS systems, such as the Drishti system~\cite{ran2004drishti}, while cheap and reliable in open outdoor environments, are not applicable here since we wish our system to be usable in built-up urban areas and indoors where GPS signals are notoriously unreliable. 

Computer vision-based systems provide a good compromise between usability, cost and accuracy and has has been the focus of much research in the recent past. One popular solution is to use an RGB-D depth sensing camera, which are becoming increasingly more accurate and cheaper, to build a 3D image map which will allow a user to safely traverse through~\cite{lee2015, rodriguez2012obstacle}. However, these systems have not yet been thoroughly tested with VI people in a real environment. Another approach is to use object recognition techniques to detect various objects and landmarks, such as doors, staircases, etc.~\cite{tian2013b}, and communicate their relative location to the user. Furthermore, though not strictly useful for navigation but nice to have, are other assistive functionalities, such as face recognition and currency reading, that can also be added to enhance the VI user's experience and add more usability to the system~\cite{chessa2016}.

It should also be noted that a large amount of the work discussed here uses a head-mounted tracking unit or camera to track where the user is looking and generate navigation instructions based on the user's gaze direction. While this approach is quite simple and keeps the user's hands free to perform other actions, it is quite cumbersome and in the way and the ideal system would require the user to rely on as few apparatus and devices as possible. We therefore propose a hand-held solution that makes use of the impressive amount of computing power available on modern smartphones and tablets. 

\subsection{Interfacing}

An important feature of a user-centric system is an effective human-machine interface (HMI) that enables effective and seamless communication between the system and the user. Creating such a system for the VI in particular can prove to be a challenge by itself. Some work has been done in this regard with a fair amount of success, from determining which feedback media the VI prefer to work on how to best translate a visual scene into a format that will be useful to a VI person. 

In their survey, \citeauthor{khoo2016multimodal}~\cite{khoo2016multimodal} found that the VI prefer receiving feedback and instructions in the form of speech and haptic feedback cues, preferring the haptic feedback to the audio feedback~\cite{ross2000wearable}, and they prefer to give instructions to the system using a familiar QWERTY (or the regional equivalent) keyboard or voice commands to query the system for output. 

On the front of user feedback, work has been done in translating a visual scene into format that would be useful to the VI, with so-called `soundscapes' (e.g. \citeauthor{meijer2010}~\cite{meijer2010} and their `The Voice' system) and virtual audio reality (VAR) systems~\cite{frauenberger2003} reporting favourable results. However, The Voice, while helpful, has a very steep learning curve that has proven to be a significant barrier to entry, and with the VAR system it is not clear how unknown environments, where markers have not yet been encoded, will be handled and described to the user. 

Spatial audio has also been considered to convey the 2D location of a target in space and experimenters have previously determined that people are able to correctly find the location of a sound source within an error range of roughly $\pm$\SI{35}{\degree} in both the pan and tilt dimension~\cite{zwiers2001spatial}. Furthermore, \citeauthor{ashmead1998spatial}~\cite{ashmead1998spatial} determined that the minimum difference in the spatial sound's angle to be able to tell whether it has moved, is approximately \SI{1.7}{\degree}. During these experiments, the sound source (a speaker in this case) was physically manoeuvred to provide the subject with a spatialised sound tone. 

Authors such as \citeauthor{holland2002audiogps}~\cite{holland2002audiogps} have tried using simulated spatial audio to instruct the VI user which direction to go in. Here a sound is played through a set of headphones and the source is spatialised with a head-related transfer function (HRTF) in order to trick the listener into thinking the sound source is located at some arbitrary 3D location. \citeauthor{holland2002audiogps} report promising results with users being able to tell the direction of the sound source on the panning plane with a fair degree of accuracy (they did not test for distance or elevation). However, their `AudioGPS' system was limited by the technology of the time and was rather cumbersome to use. 

Experimental results to determine how well users can find a target presented with spatial sound in the elevation and panning dimensions~\cite{katz2011spatial, zwiers2001spatial}, but to the authors' knowledge, no extensive work or experiments have been done with a significant sample size to determine how well users respond to elevation adjustment instructions using an audio tone with \emph{varying pitch}. 

\section{Portable Navigation System}

The system we intend to ultimately deliver is a portable navigation device that caters to the needs of the blind by using a combination of different feedback modes to facilitate two-way communication between the user and the device. A large amount of data needs to be translated from a visual form into a format that is useful to the VI. We therefore use a combination of voice, audio and vibration cues to translate the visual navigation data as effectively as possible and overcome the bandwidth limitation that is inherent to the human ear. 

The system is based on a Google Tango device, pictured in Figure~\ref{fig:tango} along with the bone-conducting headset that we have been using. A Tango device is an Android-based device available in a cellphone or tablet form-factor and comes equipped with an RGB-D camera to estimate depth and combines an inertial measurement unit (IMU) with powerful and robust landmark recognition and image processing algorithms to localise itself and `close the loop'. An added benefit of this platform is its familiar, compact form-factor which will help overcome the hurdle of user-acceptance and usability. Furthermore, this approach externalises the user's frame of reference for navigation from one relative to their head and gaze direction, commonly used by the head-mounted devices discussed in literature, to another reference frame relative to the hand holding the device and its orientation, demonstrated in Figure [SIT FIGUUR IN]. 

We use a set of bone-conducting earphones that are placed externally on the user's head, with the reason being that the system should not interfere with a user's normal hearing function. This becomes particularly important when your target user-base has significant visually impairments. 

\begin{figure}
  \centering
  \includegraphics[width=0.3\textwidth]{figures/tango_headphone.png}
  \caption{A picture of the Tango device and the bone-conducting headset.}
  \label{fig:tango}
\end{figure}

We intend to use multiple feedback modes to provide the VI user with navigation and obstacle avoidance instructions. These modes are spatialised audio, voice prompts and vibration cues. However, for this series of experiments, we only employed the spatialised audio mode in order to determine its effectiveness in conveying elevation angle to a user. We plan to use the results from the experiments we performed to better understand how the users respond to different settings for the feedback stimuli mentioned earlier, in order to improve and optimise the behaviour of the feedback modes. 

The next phase of the project is to add a co-adaptive module to the feedback interface, the goal of which is to refine the parameters of the feedback interface to better match the individual user's navigation habits and capability thereby increasing navigation performance and user satisfaction. A diagram of the entire system pipeline is shown in Figure~\ref{fig:pipeline}.

\begin{figure}
  \centering
  \includegraphics[width=0.45\textwidth]{figures/pipeline.pdf}
  \caption{A diagram of the individual system components and their communication pipelines. }
  \label{fig:pipeline}
\end{figure}

\section{Audio Interface}

For the series of experiments we performed, we only used the audio feedback mode to interface with the user. Here, the audio component is responsible for conveying the 2D position of a target relative to the target in terms of pan and tilt angles. 

The audio being generated is a sinusoidal sound wave that is constantly generated and played to the user through bone-conducting headphones. We select a sinusoidal wave due to it being relatively simple to manipulate and analyse. Furthermore, we opted to use an external, bone-conducting headset to play the audio to better simulate the use-case we are designing the system toward where our system will act as a supplementary navigation aid which does not interfere with their other senses; in particular their sense of hearing, which the VI tend to rely upon heavily. 

The audio is spatialised using the head-related transfer function (HRTF) provided by the OpenAL sound library. However, the audio is only spatialised in the pan dimension, while the tilt angle is conveyed by varying the pitch of the audio tone. We use this approach because we use an external set of bone-conducting headphones that play the sound through the user's cheekbones instead of their outer ears, bypassing the pennae of the ears which provide humans their ability to localise an elevated sound source~\cite{roffler1968factors, algazi2001elevation}, making it necessary to convey the elevation using another method. 

The angular orientation of the Tango device is used to generate the audio navigation cues. The argument has been made before in literature [CITE] that using this approach, as opposed to the head-mounted device approach, may be disorientating and unintuitive to a user. However, human beings are capable of externalising themselves and transfer their own frame of reference into an external object's and in our preliminary work, the users have not made any comments on the system being disorientating or confusing and after a brief training and familiarisation phase, has shown that they have no problem using the system and understanding the navigation cues. We would therefore argue that the head-mounted option conceptually offers no clear performance benefit over the hand-held exploration approach. 

\subsection{Pan Description}

The pan angle describes the angle which the user needs to rotate the camera vector around the Y-axis, i.e. how far the target is to the left or right of the user. To communicate this to the user, we use a head-related transfer function (HRTF) to add a spatial element to the audio tone that the system plays to the user, making the tone sound to the user like it's coming from the direction of the target. 

The HRTF functionality is implemented using the OpenAL library~\cite{hiebert2005openal} to generate a sinusoidal sound wave based on the relative difference in the user and target's positions. We implement the library as a `black box' where the inputs are positions and it outputs a tone based on the difference in those positions, implying that we don't have access to the internal parameters of the HRTF. We therefore implemented the same HRTF across all of the experiments we conducted.  

%OpenAL is able to generate a 3D spatial tone (pan, tilt and distance). However, we opted to only use it to convey the pan dimension and use other tools to convey the distance and tilt dimensions. We selected this approach partly because HRTFs commonly have difficulty communicating the tilt angle effectively, since its very reliant on the physical structure of the individual's ear and the high-frequency energy of the signal~\cite{roffler1968factors, algazi2001elevation}. 

%Furthermore, implementing other options for the distance and tilt dimensions grants us finer control over how they are communicated to the user.

\subsection{Tilt Description}

To enable the system to communicate the tilt direction of the target, the system adjusts the emitted tone's pitch as a logarithmic function of the elevation angle, $\theta$, between the user's camera vector and the target's position. Here, a high pitch means the target is above the camera vector and the user should look up, whereas a low pitch means the target is below the camera vector and the user should look down. This high/low association scheme is selected, because humans naturally tend to associate high-pitched sounds with higher objects and lower-pitched noises with lower objects~\cite{pratt1930spatial}. We also opt for a logarithmic, octave-based gain function for the pitch, since an increase in octave provides a distinct perceptible change while keeping the tone roughly similar~\cite{shepard1964circularity}.

We wish to determine what effect the gradient of the pitch gain function has on a user's performance; that is to say, does an increased rate of change in the pitch as a function of the elevation angle lead to an increased target acquisition rate, for example. For this we select three different pitch gain gradients, so-called low, median and high gain presets. To find these gradients, we set the maximum and minimum limits for the pitch and the elevation angles. Furthermore, for the sake of consistency, each gradient is set to pass through the same pitch value at the \SI{0}{\radian} elevation angle.   

The neutral, \SI{0}{\radian}, position is set to be directly in front of the user and we limit the angles between $\pm$\SI[quotient-mode=fraction]{\pi / 2}{\radian}, requiring the system to be able to communicate angles within a range of \SI{\pi}{\degree}. Anything outside of this range implies that the target is behind the user. 

After practical tests with the Tango and the headphones, we set the neutral, on-target tone to a frequency of \SI{512}{\hertz} for its audibility. For the median preset, we set the maximum and minimum pitches to be two octaves higher and lower than the neutral tone, giving limits of \SI{2048}{\hertz} and \SI{128}{\hertz} respectively. The low preset is set to one octave higher and lower than the neutral tone (\SI{1024}{\hertz} and \SI{256}{\hertz}) and the hight to 3 octaves higher and lower (\SI{4096}{\hertz} and \SI{64}{\hertz} than the neutral tone. We selected these limits partially more practical reasons, given the fact that the bone conducting headphones we used have low volume gain at very high and low frequencies, making it difficult to hear. Figure~\ref{fig:pitch-preset-plot} shows the low, median and high gain preset graphs.  

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{figures/pitch_gradient.pdf}
  \caption{Plot depicting the different pitch gain preset functions.}
  \label{fig:pitch-preset-plot}
\end{figure}

\section{Experiments}

To determine how effective the individual feedback modes of our HMI is at directing a user to perform a given task, we performed a set of experiments with blindfolded users using a limited set of the feedback modes. The reason for only experimenting with one or two modes together is to simplify the experiment procedure and incrementally build up our knowledge of the interactions between the user and the feedback mode so that we can eventually integrate all of the feedback modes into a single implementation and perform an optimal set of experiments that will provide us with all of the important data that we require. 

In this case, we experimented with the spatialised sound feedback mode; that is to say we determined how effective a spatial tone, with varying pitch, is at directing a user to pan and tilt a camera to find a target. Furthermore, we also carried out a set of pre-screening experiments to determine each subject's hearing characteristics. The following sections will discuss all of these experiments and their objectives in detail. 

\subsection{Experiment Objectives}

Every subject was asked to participate in 4 different experiments: the first three were pre-screening experiments to evaluate the subjects' hearing characteristics and the last to determine how effective our HMI's spatial sound is at controlling a subject's pan and tilt. Furthermore, we wish to determine how the different values of this spatial sound affects a subject's performance. 

The hearing characteristics we wished to determine were the subjects' spatial awareness, tone limits and tone discrimination capability. For the spatial sound experiment we wish to determine how quickly each subject can find their target's, what each subject's target search strategy is as well as how the different feedback parameter values affect these performances. 

\subsection{Experimental Procedure}

For the experiments we used 40 sighted and blindfolded volunteers and had them perform a series of experiments using our system and a pair of bone conducting headphones. The subjects were recruited on a volunteer-basis and consisted of a diverse group of undergraduate students with ages ranging between [WHAT ARE THE AGES?], with [WHAT ARE THE GENDER NUMBERS?]. The subjects also reported having no significant sight or hearing issues or any other major disability. 

The 40 subjects were asked to participate in 4 experiments, each of which is discussed here. The first 3 experiments were performed to determine each subject's hearing characteristics and capabilities to provide some context to the results generated during the 4th and final target-search experiment. 

\subsubsection{Subject Characterisation}\hfill

\textbf{Spatial Awareness Experiment}: In this experiment, we determine a subject's ability to tell the direction a sound is coming from. To do this, we play a 512Hz sinusoidal tone to the subject through the headphones that comes from either the left or right of the subject. The subject must then select whether the sound source is to the left or right. The sound source location is simulated using a head-related transfer function (HRTF). The longer this experiment is run, the source moves closer to the centre of the subject making it more difficult to localise the sound source. 

For this progressive increase in difficulty, a 2-up, 1-down step process is used, meaning that for every 2 correct answers, the distance to the centre halves, making the process harder. Conversely, it becomes easier for each incorrect answer by doubling the sound source's distance from the centre. We also select to use 2 different step sequences, one starting at a large distance (\SI{2}{\m})[CHECK] from the user and the other at a close distance (\SI{0.125}{\m}) [CHECK], giving us an `easy' and `hard' step respectively. The terminating condition for the experiment is when the 2 step sequences are within 2 step ranges of each other for 3 consecutive guesses. This will give us a distance band within which the subject is capable of localising the sound source. Each subject will performed this experiment three times. 

\textbf{Pitch Discrimination Experiment}: For this experiment , we determine a subject's ability to tell tones apart, i.e. how well can they tell if a tone is high or low pitched? Here we play 2 tones to the subjects, one after the other, with one tone being higher or lower-pitched than the other. The subject's were asked to select whether the second tone was higher or lower-pitched than the first tone.

One tone was randomly generated by the app and the second tone was generated by adding or subtracting the difference from the first tone. This difference is based on an exponential function, $f(n) = 2^2$, where $n$ was increased or decreased to adjust the differentiation difficulty. 

As with the spatial experiment, a 2-up, 1-down step process is used: for every 2 consecutive correct answers, the pitch difference between the two tones will be halved, increasing the difficulty, and the difference is doubled, i.e $n$ is incremented by 1, for every incorrect answer, making the tones easier to differentiate. Two step sequences are again used here, one starting with a large pitch difference (\SI{1024}{\hertz})[CHECK] between the tones and the other with a small difference (\SI{2}{\hertz}) [CHECK]. The termination condition is when the two step sequences are within one octave of each other for 3 consecutive answers. Each subject performed this experiment twice. 

\textbf{Tone Limit Experiment}: We determined the subjects' tone limits as a final experiment before they took part in the  main, target-finding experiment. We did this by playing a single tone that increases in pitch as time progresses. The subject was then asked to click a button as soon as he/she started hearing a tone and to click the button again when the tone became inaudible. The subject was then asked to repeat the process 6 times, but the tone direction was reversed after each run, meaning that the tone either started high and went low or started low and went high. 

\subsubsection{Target Search Experiment}\hfill

The final experiment is the main one and will answer the question we are most interested in: how well does a spatial tone direct a user to look in a specific direction, and how do the parameters of this tone affect the user's performance in this task? 

Here an experiment subject was blindfolded and given a Tango device running an app written specifically for this experiment. When started, a set of virtual targets were presented one at a time to the subject on the Tango's screen. Then, depending on the direction the subject is currently pointing the camera relative to the target's position, the Tango generates and plays a tone via a bone-conducting headphone to indicate the pan and tilt adjustment the subject needs to make the camera to face the target. These instructions are a spatialised tone with varying pitch: an HRTF will indicate whether the target is to the left or the right and the pitch will indicate whether the subject should be looking up (high pitch) or down (low pitch) to find the target. 

Once the subject pointed the camera toward the target, the HRTF centred the tone in front of the subject with a neutral pitch of 512Hz, which we used as the `on-target' pitch for all of our experiment (the subjects were given a few minutes without a blindfold where they could familiarise themselves with the system where they could confirm the target's location with their own eyes). However, the subject had to decide for themselves whether they truly were looking at the target and tap the screen to indicate the location they believe the target to be in (i.e. the current location they are looking at). At this point a new target is presented to the subject which they had to search for again. 28 targets are presented to each subject per round. 

After every round of these experiments, the parameters controlling the tone's behaviour were adjusted. In this case, the rate of change of the tone's pitch was adjusted to make the pitch increase at a lower or higher rate as a function of the elevation angle between the target and the subject's current looking direction. This was done to see whether, for example, a more rapid increase in pitch will help the subject find the target faster. 

For this experiment, the distance between the subject and the target is not considered here. Therefore, the target's are generated on a plane at a constant distance from the subject, in this case 2m. Throughout the experiment, various parameters of the target and the subject are recorded and streamed in real-time to a laptop computer via a WiFi connection.

\section{Results}

\subsection{Preliminary Experiment}

The first experiment that each test subject participated in was the tone differentiation experiment. The results recorded during this experiment is shown in Figure~\ref{fig:tone-guesses} where a bar plot is used to show the frequency of correct guesses versus the incorrect guesses for each tone difference level. 

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{figures/tone_guesses.png}
  \caption{A bar plot showing the subjects' guesses about the tone differences [SIT HATCH IN FIG].}
  \label{fig:tone-guesses}
\end{figure}

As one might expect, the frequency for the extreme, larger tone differences is significantly higher than for the smaller differences, where around 80\% of the respondents' correct answers were given at a difference of \SI{16}{\hertz} or more, with the reason being that it is easier to discriminate between two tones with a large tone difference. The number of incorrect guesses in this interval is approximately 50\% of the incorrect answers. For frequencies less than \SI{16}{\hertz}, the number of correct guesses drops down to 20\% of the total correct guesses while the incorrect guesses remains around 50\%. This indicates that our subjects' incorrect answers are fairly consistent across the frequency spectrum, but correct tonal discrimination is far more likely to occur with frequency differences higher than \SI{16}{\hertz}.  

These results are in line with what one might expect, where it becomes increasingly difficult to differentiate between two tones with a small difference for a typical user. However, these results are useful for our research when we begin implementing autonomous adaptation capabilities into our system. 

Figure~\ref{fig:location-guesses} shows the results for the spatial awareness experiment, where the subjects' had to determine the location of the sound they were played. 

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{figures/location_guesses.png}
  \caption{A bar plot showing the subjects' guesses about the tone's location.}
  \label{fig:location-guesses}
\end{figure}

In Figure~\ref{fig:location-guesses}, with the relatively low number of red, incorrect guesses, we see that the subjects were far more successful in correctly guessing the location of the sound source. This is further supported by the large number of samples at the \SI{0.03125}{\m}, which was the minimum achievable level for this experiment, indicating that the subjects reached this level more frequently and consistently. Here we can see that the subjects had little problem localising the left-right direction of a sound source. 

Again, these results are in line with what we expected and is supported by literature which indicates that humans are very adept at localising the location of a sound source and this ability was apparent for HRTF-generated pan location. [CITE]

\subsection{Target Search Experiment}

\subsubsection{Tilt Dimension Results}

Figures~\ref{fig:tilt-err-lo} to~\ref{fig:tilt-err-hi} shows the results recorded during the target search experiment for the tilt dimension. Plots are presented for each of the three pitch gain gradient's, i.e. `lo', `med' and `hi'. For each data set, a scatter plot of the user's guesses of where the target is vs. the target's actual location is given, along with a linear regression plot to demonstrate the correlation between the data sets. A box-plot is also given to convey the average angular error between the user's guess and the target along with the spread of this angular error. The absolute errors are given on the plots.

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{figures/tilt_err_lo.png}
  \caption{A scatter plot showing the user's guess vs the actual position of the target's location, along with the corresponding box plots for the `lo' gradient configuration. The Pearson correlation coefficient is 0.34. }
  \label{fig:tilt-err-lo}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{figures/tilt_err_med.png}
  \caption{A scatter plot showing the user's guess vs the actual position of the target's location, along with the corresponding box plots for the `med' gradient configuration. The Pearson correlation coefficient is 0.44. }
  \label{fig:tilt-err-med}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{figures/tilt_err_hi.png}
  \caption{A scatter plot showing the user's guess vs the actual position of the target's location, along with the corresponding box plots for the `hi' gradient configuration. The Pearson correlation coefficient is 0.48. }
  \label{fig:tilt-err-hi}
\end{figure}

We can see from the plots that there is a significant linear correlation between the user's guesses and the actual locations of the targets, indicating that the varying pitch is working as expected and the users in general are interpreting the cues correctly. This is supported by the Pearson correlation coefficient of the datasets that equate to 0.34, 0.44 and 0.48 for the lo, med and hi configurations respectively, each with a statistical significance lower than 5\%, indicating that it is reasonable to trust the correlation scores. The Spearman correlation scores are 0.40, 0.52, 0.55 for each respective dataset with a significance level still below 5\%. 

The average errors of the datasets are closer to each other, with the lo gradient giving the largest absolute error at \SI{22.6}{\degree} and the med and hi a similar absolute error of \SI{18.4}{\degree} and \SI{19.3}{\degree} respectively. This is in line with the correlation scores with the lo gradient giving a significantly worse result than the med and hi gradients and the hi gradient giving the best results overall with its high correlation score and relatively low absolute angular error. 

The data is not normally spread and displays a significant skewing to the negative side indicating a potential bias amongst the experiment subject-base that must be taken into account. We might have to consider using a non-linear increase in pitch as a function of elevation angle instead of the linear one we used for these experiments to remedy this bias.

These results highlight a clear and significant difference between the three different pitch gain gradients, with the steep-gradient gain in pitch as a function of target elevation angle producing the results closest to the true elevation. Moreover, it shows that a tone with varying pitch can be used to convey the elevation angle of a virtual target to a human user to a degree of accuracy similar to those previously established in literature~\cite{bujacz2011sonification, katz2011spatial, zotkin2004rendering} using spatial sound, either with an HRTF or mobile speakers, and a head-mounted orientation tracker.  

\subsubsection{Panning Dimension Results}

The results from the target search experiment in the pan dimension are given in Figures~\ref{fig:pan-err-lo} to~\ref{fig:pan-err-hi}, where a scatter plot of the subjects' guesses vs. the target's actual location are given along with a box-plot of the angular errors between the guesses and actual locations of the targets. A plot for each of the `lo', `med' and `hi' configurations are given. The Pearson correlation and average errors for each dataset are given with the corresponding plot. 

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{figures/pan_err_lo.png}
  \caption{A plot showing a histogram of the pan errors, along with the corresponding box plots. }
  \label{fig:pan-err-lo}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{figures/pan_err_med.png}
  \caption{A plot showing a histogram of the pan errors, along with the corresponding box plots. }
  \label{fig:pan-err-med}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{figures/pan_err_hi.png}
  \caption{A plot showing a histogram of the pan errors, along with the corresponding box plots. }
  \label{fig:pan-err-hi}
\end{figure}

The scatter plots display evidence of a linear correlation between the subjects' guesses and the targets' true locations. This is confirmed by both the Pearson and Spearman correlation scores of above 0.7 for all three datasets, with the med configuration displaying the best result at 0.75 for the Pearson score and 0.8 for the Pearson, with a statistical level of significance well below 5\%. 

The angular errors are roughly normally distributed around zero with comparable average errors and standard deviations, with the med configuration again producing the best results with the smallest average error and standard deviation. 

Based on previous research results[CITE], these results were somewhat expected. However, they do confirm that the target search capability of a subject in the pan dimension is fairly robust to the changing pitch we used to convey the target's elevation error which is a useful result going forward. The smallest angular error and highest correlation was achieved when the med gain gradient for the pitch was used. However, the difference between these values here is not significant enough to conclude that the varying pitch has a significant effect on the accuracy of the target search in the pan dimension. 

\subsubsection{Time to Target}

The performance between the three difference pitch gradient configurations can be compared using different metrics. The previous sections established the accuracy difference between the configurations. Here we compare the performance in terms of the time it took each subject to find a target. 

For this analysis we opt to use Fitts's Law~\cite{fitts1954information}, which states that there is a logarithmic relation between the time it took to find a target and the difficulty of finding the target, i.e. its difficulty index. It also gives us a so-called `index of performance' that we can use as a metric to compare the results between the three configurations. Fitts's Law is given in Equation~\ref{eq:fitts-base}. 

\begin{equation}
  \label{eq:fitts-base}
  t = a + b\log_2\left(\frac{d}{w} + 1\right)
\end{equation}

Here, $t$ is the time it takes to find the target and $d$ is the distance between the two subsequent targets' centre points, while $w$ is the width of the target and $a$ and $b$ are constants determined through regression.

Fitts's Law uses the target's width as a parameter in its index of difficulty. However, our targets do not have a width, which means we have to use an effective width, $w_e$, as a parameter instead. Using this parameter was originally proposed by \citeauthor{mackenzie1992fitts}~\cite{mackenzie1992fitts} in order to provide more accurate results when the process contains noise and uncertainties. In our case, the noise is introduced by the subjects not selecting the target at a constant distance from the target's centre. 

$w_e$ is given by 

\[
  w_e = \sqrt{2\pi e}\sigma = 4.133\sigma
\]

where $\sigma$ is the standard deviation in the error data, taken as the difference in Euclidian distance between the subjects' target selections and the targets' actual positions. This results in the modified form of Fitts's Law, given in Equation~\ref{eq:fitts-simple}.

\begin{equation}
  \label{eq:fitts-simple}
  t = a + b\log_2\left(\frac{d}{4.133\sigma} + 1\right) = a + b ID
\end{equation}

Here $ID$ is shorthand for the Index of Difficulty. We used the relation from Equation~\ref{eq:fitts-simple} and fitted a line through the distance-time data we gathered. The resulting plots are given in Figures~\ref{fig:fitts-lo} to~\ref{fig:fitts-hi}. 

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{figures/fitts_lo.png}
  \caption{A plot showing the data for the time it took for the user to point the Tango to where they think the next target was in the `lo' configuration. }
  \label{fig:fitts-lo}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{figures/fitts_med.png}
  \caption{A plot showing the data for the time it took for the user to point the Tango to where they think the next target was in the `med' configuration. }
  \label{fig:fitts-med}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{figures/fitts_hi.png}
  \caption{A plot showing the data for the time it took for the user to point the Tango to where they think the next target was in the `hi' configuration. }
  \label{fig:fitts-hi}
\end{figure}

From these figures we can see that the data roughly obeys Fitts's Law and forms a logarithmic graph. This enables us to use the index of performance proposed by Fitts~\cite[p.~390]{fitts1954information}, given by

\[
  IP = \frac{ID}{t}
\]

We used this relation to plot Figure~\ref{fig:fitts-performance}.

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{figures/fitts_performance.png}
  \caption{A plot comparing the indices of performance for the three different pitch gradient configurations.}
  \label{fig:fitts-performance}
\end{figure}

From Figure~\ref{fig:fitts-performance} that the `lo' and `med' configurations give similar results with the `hi' configuration gives the best result, where it took the average subject between 7 to 9 seconds less to find the most difficult target with the `hi' configuration. 

\section{Future Work and Conclusion}

\section{Acknowledgements}

We would like to thank Prof. Iain Gilchrest and his team from the University of Bristol's Department of Experimental Psychology for their help and support in designing these experiments, gathering and analysing the data. 

\bibliographystyle{ACM-Reference-Format}
\bibliography{icmi17}

\end{document}
